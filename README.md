# LLM-Computing

This repository contains a small experiment for training a GPT-style model whose attention
weights are provided externally rather than learned. The attention scores are generated by
the OpenAI API and cached locally. The code is intentionally minimal so that developers can
easily modify and extend it.

## Repository layout

- `attention/` – utilities for requesting attention scores from the OpenAI API.
  - `scorer.py` defines `get_attention_matrix()` which returns a softmax-normalised
    attention tensor.
- `model/` – the model implementation.
  - `attention.py` provides the `LLMAttention` module which consumes an attention
    matrix and applies it to value vectors.
  - `attn_gpt.py` implements a very small GPT architecture that relies on
    `LLMAttention`.
- `train.py` – builds a vocabulary from `data/corpus.txt` and trains `AttnGPT` using
  the external attention scores.
- `sample.py` – loads `checkpoint.pt` produced during training and offers an
  interactive prompt for text generation.
- `data/` – directory for your training corpus. A short `example.txt` is included
  as a reference.

## Requirements

See `requirements.txt` for package versions. A Python 3.10+ environment is
recommended. Install the dependencies with:

```bash
pip install -r requirements.txt
```

`attention/scorer.py` loads your OpenAI API key from the environment, so set
`OPENAI_API_KEY` before running any training or sampling scripts.

## Training

1. Place a plain text corpus at `data/corpus.txt`, one sentence per line.
2. Run the training script:

```bash
python train.py
```

`train.py` will repeatedly choose random sentences from the corpus, obtain
attention matrices using the OpenAI API, and train the model. Every 50 epochs it
prints the current loss.

## Sampling

After training, a checkpoint named `checkpoint.pt` will be saved in the project
root. Launch the sample script to generate text interactively:

```bash
python sample.py
```

Enter a prompt and the model will autoregressively generate additional tokens.
The script reuses the vocabulary and device settings from `train.py` and uses
`get_attention_matrix()` at inference time.

## Notes for development

- `scorer.py` shows where the API key is loaded and the OpenAI client is
  initialised:
  ```python
  load_dotenv()                          # loads OPENAI_API_KEY
  client = AsyncOpenAI()
  ```
  【F:attention/scorer.py†L10-L15】
- `train.py` expects the corpus at `data/corpus.txt` and tokenises each line
  using a simple regex:
  ```python
  corpus_path = Path("data/corpus.txt")
  ...
  return re.findall(r"\w+|\.", line.strip().lower())
  ```
  【F:train.py†L13-L19】
- The training loop constructs attention tensors for each sentence and performs a
  teacher-forced next-token prediction task:
  ```python
  att = get_attention_matrix(tuple(tokens)).to(device)  # (T, T)
  ...
  logits = model(idx_in, att_in)
  ```
  【F:train.py†L36-L52】

Developers can modify these scripts to experiment with alternative data sets,
model sizes, or attention generation strategies.

## License

This project is licensed under the Apache 2.0 License. See `LICENSE` for full
details.

