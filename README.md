# LLM-Computing

This repository contains a small experiment for training a GPT-style model whose attention
weights are provided externally rather than learned. The attention scores are generated by
the OpenAI API and cached locally. The code is intentionally minimal so that developers can
easily modify and extend it.

## Repository layout

- `attention/` – utilities for requesting attention scores from the OpenAI API.
  - `scorer.py` defines `get_attention_matrix()` which returns a softmax-normalised
    attention tensor.
- `model/` – the model implementation.
  - `attention.py` provides the `LLMAttention` module which consumes an attention
    matrix and applies it to value vectors.
  - `attn_gpt.py` implements a very small GPT architecture that relies on
    `LLMAttention`.
- `train.py` – builds a vocabulary from `data/corpus.txt` and trains `AttnGPT` using
  the external attention scores.
- `sample.py` – loads `checkpoint.pt` produced during training and offers an
  interactive prompt for text generation.
- `data/` – directory for your training corpus. A short `example.txt` is included
  as a reference.

## Requirements

See `requirements.txt` for package versions. A Python 3.10+ environment is
recommended. Install the dependencies with:

```bash
pip install -r requirements.txt
```

`attention/scorer.py` reads provider settings from the environment. Set
`LLM_PROVIDER` and the appropriate API key(s) such as `OPENAI_API_KEY` or
`CEREBRAS_API_KEY` before running any training or sampling scripts.

### Provider configuration

The attention scorer supports multiple back-end providers. Set
`LLM_PROVIDER` to select one of the following:

- `openai` – authenticates using `OPENAI_API_KEY`.
- `cerebras` – uses `CEREBRAS_API_KEY` and optional `CEREBRAS_BASE_URL`.
- `custom` – any OpenAI-compatible endpoint via `CUSTOM_BASE_URL` and
  `CUSTOM_API_KEY`.

The bootstrap logic in `scorer.py` illustrates how the provider is chosen:
```python
load_dotenv()
PROVIDER = os.getenv("LLM_PROVIDER", "cerebras").lower()
...
```
【F:attention/scorer.py†L29-L63】

## Training

1. Place a plain text corpus at `data/corpus.txt`, one sentence per line.
2. Run the training script:

```bash
python train.py
```

`train.py` will repeatedly choose random sentences from the corpus, obtain
attention matrices using the OpenAI API, and train the model. Every 50 epochs it
prints the current loss.

## Sampling

After training, a checkpoint named `checkpoint.pt` will be saved in the project
root. Launch the sample script to generate text interactively:

```bash
python sample.py
```

Enter a prompt and the model will autoregressively generate additional tokens.
The script reuses the vocabulary and device settings from `train.py` and uses
`get_attention_matrix()` at inference time.

## Attention caching

From `v0.2` onwards the attention scorer stores LLM-evaluated token pair scores
in a global cache. Repeated pairs across training or sampling will therefore
reuse the cached value instead of issuing a new API request. This reduces both
latency and API cost for long sequences.

## Notes for development

- `scorer.py` demonstrates provider bootstrapping via environment variables:
  ```python
  load_dotenv()
  PROVIDER = os.getenv("LLM_PROVIDER", "cerebras").lower()
  ...
  ```
  【F:attention/scorer.py†L29-L63】
- `train.py` expects the corpus at `data/corpus.txt` and tokenises each line
  using a simple regex:
  ```python
  corpus_path = Path("data/corpus.txt")
  ...
  return re.findall(r"\w+|\.", line.strip().lower())
  ```
  【F:train.py†L13-L19】
- The training loop constructs attention tensors for each sentence and performs a
  teacher-forced next-token prediction task:
  ```python
  att = get_attention_matrix(tuple(tokens)).to(device)  # (T, T)
  ...
  logits = model(idx_in, att_in)
  ```
  【F:train.py†L36-L52】

Developers can modify these scripts to experiment with alternative data sets,
model sizes, or attention generation strategies.

## License

This project is licensed under the Apache 2.0 License. See `LICENSE` for full
details.

